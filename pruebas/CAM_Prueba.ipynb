{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CAM_Prueba.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6988ed00ea034680b52cc815b06dd736": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a7e83b248ee04f52a6525423b61834f3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_64509813695c4806be8ce7fa842fcfd4",
              "IPY_MODEL_f41aaef0bb5f44bcaf6076ed419655a4",
              "IPY_MODEL_d98c560a0bee41f1b32a1dc81deeb73c"
            ]
          }
        },
        "a7e83b248ee04f52a6525423b61834f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "64509813695c4806be8ce7fa842fcfd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c3861722e6f24e0fa4691d8fbff30a91",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6586d25b077b41759321f60ef926b96e"
          }
        },
        "f41aaef0bb5f44bcaf6076ed419655a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d111ddc1620e4055876b6b1b13b0b2ff",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d56cd4603e4f4501bc384deee482eb21"
          }
        },
        "d98c560a0bee41f1b32a1dc81deeb73c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2bd58f35081f43709791a84aeeb24783",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 528M/528M [00:04&lt;00:00, 137MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_510466537e324cc29c3b6e7cbb4b0c9c"
          }
        },
        "c3861722e6f24e0fa4691d8fbff30a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6586d25b077b41759321f60ef926b96e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d111ddc1620e4055876b6b1b13b0b2ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d56cd4603e4f4501bc384deee482eb21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2bd58f35081f43709791a84aeeb24783": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "510466537e324cc29c3b6e7cbb4b0c9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# En primer lugar cargamos nuestro Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --NotebookApp.port_retries=0 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCw7KTG05NzX",
        "outputId": "fccc8587-a779-4371-cc3f-281f53e9e8ba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UB0gXebrUkat"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules.pooling import AdaptiveAvgPool2d\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "import torchvision.models as models\n",
        "import torchvision\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_vgg = models.vgg16(pretrained=True)\n",
        "print(model_vgg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847,
          "referenced_widgets": [
            "6988ed00ea034680b52cc815b06dd736",
            "a7e83b248ee04f52a6525423b61834f3",
            "64509813695c4806be8ce7fa842fcfd4",
            "f41aaef0bb5f44bcaf6076ed419655a4",
            "d98c560a0bee41f1b32a1dc81deeb73c",
            "c3861722e6f24e0fa4691d8fbff30a91",
            "6586d25b077b41759321f60ef926b96e",
            "d111ddc1620e4055876b6b1b13b0b2ff",
            "d56cd4603e4f4501bc384deee482eb21",
            "2bd58f35081f43709791a84aeeb24783",
            "510466537e324cc29c3b6e7cbb4b0c9c"
          ]
        },
        "id": "x6Dtl-x_V0nP",
        "outputId": "3ad9c17c-49e1-44b3-8978-32099a9c1acb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6988ed00ea034680b52cc815b06dd736",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/528M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pool of non-square window\n",
        "m = nn.AdaptiveAvgPool2d(output_size=(7,7))\n",
        "lin = nn.Linear(in_features=512, out_features = 4096, bias = True)\n",
        "mm = nn.AvgPool2d(20)\n",
        "l=[]\n",
        "for i in range(20):\n",
        "  row=[]\n",
        "  for j in range(20):\n",
        "    row.append(float(20*i+j))\n",
        "  l.append(row)\n",
        "\n",
        "\n",
        "arr=np.array([l])\n",
        "input = torch.from_numpy(arr)\n",
        "input = torch.rand(512,30,30)\n",
        "\"\"\"\n",
        "# Ejecutamos los pooling\n",
        "output = m(input)\n",
        "print(f\"tensor:\\n {output}\\n\\nOutput shape: {output.shape}\")\n",
        "output2 = lin(torch.reshape(output, (1,25088)))\n",
        "print(f\"tensor:\\n {output2}\\n\\nOutput shape: {output2.shape}\")\n",
        "\"\"\"\n",
        "# RuntimeError: mat1 and mat2 shapes cannot be multiplied (3584x7 and 25088x4096)\n",
        "\n",
        "output = mm(input)\n",
        "#print(f\"tensor:\\n {output}\\n\\nOutput shape: {output.shape}\")"
      ],
      "metadata": {
        "id": "th_ElIynLF9B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG16_CAM(nn.Module):\n",
        "  def __init__(self, originalModel, D_out):\n",
        "    super(VGG16_CAM, self).__init__()\n",
        "    self.features = nn.Sequential(*list(originalModel.features)) \n",
        "    self.avgPool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
        "    self.fc = nn.Linear(in_features=512, out_features=D_out, bias=False)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x=self.features(x)\n",
        "    x=self.avgPool(x)\n",
        "    sh=x.shape\n",
        "    x=torch.reshape(x,(sh[0],sh[1]))\n",
        "    x=self.fc(x)\n",
        "    return x\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "j0tugFeK8IPc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = nn.Softmax(dim=-1)\n",
        "mm = nn.Softmax(dim=1)\n",
        "input = torch.randn(2, 3)\n",
        "output1 = m(input)\n",
        "print(input)\n",
        "print()\n",
        "print(output1)\n",
        "print(np.sum(output1.numpy(), axis=0))\n",
        "\n",
        "output2 = mm(input)\n",
        "print()\n",
        "print(output2)\n",
        "print(np.sum(output2.numpy(), axis=0))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvIs8xxTcNCo",
        "outputId": "7797e837-cbbb-44c7-88ec-33b899a2e765"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.8232,  0.1547,  0.8142],\n",
            "        [ 0.5793, -0.4046,  0.2903]])\n",
            "\n",
            "tensor([[0.3995, 0.2047, 0.3958],\n",
            "        [0.4711, 0.1761, 0.3528]])\n",
            "[0.870507   0.38081628 0.7486768 ]\n",
            "\n",
            "tensor([[0.3995, 0.2047, 0.3958],\n",
            "        [0.4711, 0.1761, 0.3528]])\n",
            "[0.870507   0.38081628 0.7486768 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREPARAMOS LOS DIRECTORIOS DE LOS DATOS\n",
        "Puesto que carecemos de archivo de etiquetas y las etiquetas vienen en el nombre, es necesario organizar los archivos en carpetas de /dogs y /cats para poder almacenar las etiquetas en pytorch (tendrá un valor u otro según de donde leamos y posteriormente se hará un shuffle)"
      ],
      "metadata": {
        "id": "_UHz-LePfxBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver la página\n",
        "# https://medium.com/predict/using-pytorch-for-kaggles-famous-dogs-vs-cats-challenge-part-1-preprocessing-and-training-407017e1a10c"
      ],
      "metadata": {
        "id": "MHswSKkzet7l"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"drive/MyDrive/Cats_vs_Dogs\"\n",
        "\"\"\"\n",
        "!ls {path}\n",
        "!ls {path}/test/cats | wc -l\n",
        "!ls {path}/test/dogs | wc -l\n",
        "!ls {path}/train/cats | wc -l\n",
        "!ls {path}/train/dogs | wc -l\n",
        "!ls {path}/val/cats | wc -l\n",
        "!ls {path}/val/dogs | wc -l\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "BOzR5gmpqvzB",
        "outputId": "bd283b02-15a0-4988-f156-96b5b0028f5a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!ls {path}\\n!ls {path}/test/cats | wc -l\\n!ls {path}/test/dogs | wc -l\\n!ls {path}/train/cats | wc -l\\n!ls {path}/train/dogs | wc -l\\n!ls {path}/val/cats | wc -l\\n!ls {path}/val/dogs | wc -l\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomRotation(5),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomResizedCrop(224, scale=(0.96, 1.0), ratio=(0.95, 1.05)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize([224,224]),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize([224,224]),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "}\n",
        "\n",
        "data_dir = 'drive/MyDrive/Cats_vs_Dogs'\n",
        "CHECK_POINT_PATH = 'drive/MyDrive/Cats_vs_Dogs/checkpoint.tar'\n",
        "SUBMISSION_FILE = 'submission.csv'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n",
        "                                              shuffle=True, num_workers=2)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "dataset_test = datasets.ImageFolder(os.path.join(data_dir, 'test'), data_transforms['test'])\n",
        "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, \n",
        "                                              shuffle=True, num_workers=1)\n",
        "\n",
        "\n",
        "class_names = image_datasets['train'].classes\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(class_names) # => ['cats', 'dogs']\n",
        "print(f'Train image size: {dataset_sizes[\"train\"]}')\n",
        "print(f'Validation image size: {dataset_sizes[\"val\"]}')\n",
        "print(f'Test image size: {len(dataset_test)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKvlYPTS9_tJ",
        "outputId": "d0570fc6-1a69-4784-eddf-771b292d70ed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cats', 'dogs']\n",
            "Train image size: 18000\n",
            "Validation image size: 2000\n",
            "Test image size: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def imshow(inp, title=None):\n",
        "    #Imshow for Tensor.\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "# Make a grid from batch\n",
        "sample_train_images = torchvision.utils.make_grid(inputs)\n",
        "imshow(sample_train_images, title=classes)\n",
        "\"\"\"\n",
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "print(inputs[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piVSefOcBVrh",
        "outputId": "37775669-beac-4481-be5e-cfedf2863758"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=2, checkpoint = None):\n",
        "  since = time.time()\n",
        "  if checkpoint is None:\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = math.inf\n",
        "    best_acc = 0.\n",
        "  else:\n",
        "    print(f'Val loss: {checkpoint[\"best_val_loss\"]}, Val accuracy: {checkpoint[\"best_val_accuracy\"]}')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    best_loss = checkpoint['best_val_loss']\n",
        "    best_acc = checkpoint['best_val_accuracy']\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    print('-' * 10)\n",
        "    \n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            scheduler.step()\n",
        "            model.train()  # Set model to training mode\n",
        "        else:\n",
        "            model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "    \n",
        "        # Iterate over data.\n",
        "        for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          if i % 200 == 199:\n",
        "              print('[%d, %d] loss: %.3f' % \n",
        "                    (epoch + 1, i, running_loss / (i * inputs.size(0))))\n",
        "\n",
        "          # forward\n",
        "          # track history if only in train\n",
        "          with torch.set_grad_enabled(phase == 'train'):\n",
        "              outputs = model(inputs)\n",
        "              _, preds = torch.max(outputs, 1)\n",
        "              loss = criterion(outputs, labels)\n",
        "              \n",
        "              # backward + optimize only if in training phase\n",
        "              if phase == 'train':\n",
        "                  loss.backward()\n",
        "                  optimizer.step()\n",
        "          \n",
        "          # statistics\n",
        "          running_loss += loss.item() * inputs.size(0)\n",
        "          running_corrects += torch.sum(preds == labels.data)\n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "        \n",
        "        # deep copy the model\n",
        "        if phase == 'val' and epoch_loss < best_loss:\n",
        "            print(f'New best model found!')\n",
        "            print(f'New record loss: {epoch_loss}, previous record loss: {best_loss}')\n",
        "            best_loss = epoch_loss\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        print()\n",
        "\n",
        "  time_elapsed = time.time() - since\n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "  print('Best val Acc: {:.4f} Best val loss: {:.4f}'.format(best_acc, best_loss))\n",
        "\n",
        "  # load best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "    \n",
        "  return model, best_loss, best_acc\n",
        "\n",
        "\n",
        "\n",
        "model_conv = VGG16_CAM(model_vgg, D_out=2)\n",
        "#model_conv = model_vgg\n",
        "\n",
        "\n",
        "model_conv = model_conv.to(device)\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized\n",
        "optimizer_conv = optim.SGD(model_conv.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "try:\n",
        "    checkpoint = torch.load(CHECK_POINT_PATH)\n",
        "    print(\"checkpoint loaded\")\n",
        "except:\n",
        "    checkpoint = None\n",
        "    print(\"checkpoint not found\")\n",
        "\n",
        "\"\"\"\n",
        "#model_conv, best_val_loss, best_val_acc = train_model(model_conv,\n",
        "                                                      criterion,\n",
        "                                                      optimizer_conv,\n",
        "                                                      exp_lr_scheduler,\n",
        "                                                      num_epochs = 3,\n",
        "                                                      checkpoint = checkpoint)\n",
        "#torch.save({'model_state_dict': model_conv.state_dict(),\n",
        "            'optimizer_state_dict': optimizer_conv.state_dict(),\n",
        "            'best_val_loss': best_val_loss,\n",
        "            'best_val_accuracy': best_val_acc,\n",
        "            'scheduler_state_dict' : exp_lr_scheduler.state_dict(),\n",
        "            }, CHECK_POINT_PATH)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "AvigJAG-D2-B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "eac21688-7f93-4f5b-d1c1-eeb3e2768164"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint loaded\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n#model_conv, best_val_loss, best_val_acc = train_model(model_conv,\\n                                                      criterion,\\n                                                      optimizer_conv,\\n                                                      exp_lr_scheduler,\\n                                                      num_epochs = 3,\\n                                                      checkpoint = checkpoint)\\n#torch.save({'model_state_dict': model_conv.state_dict(),\\n            'optimizer_state_dict': optimizer_conv.state_dict(),\\n            'best_val_loss': best_val_loss,\\n            'best_val_accuracy': best_val_acc,\\n            'scheduler_state_dict' : exp_lr_scheduler.state_dict(),\\n            }, CHECK_POINT_PATH)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_conv = torch.load(\"drive/MyDrive/Cats_vs_Dogs/model.pth\")\n",
        "\n",
        "\n",
        "model_conv = model_conv.to(device)\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized\n",
        "optimizer_conv = optim.SGD(model_conv.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "try:\n",
        "    checkpoint = torch.load(CHECK_POINT_PATH)\n",
        "    print(\"checkpoint loaded\")\n",
        "except:\n",
        "    checkpoint = None\n",
        "    print(\"checkpoint not found\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1WVECm0sdpN",
        "outputId": "6ebd3e2c-99f9-429b-bfe0-a0d48bd81302"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_r8o8s-YSGWT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cogemos la activación de las capas\n",
        "x, classes=next(iter(dataloader_test))\n",
        "x=x.to(device)\n",
        "classes=classes.to(device)\n",
        "\n",
        "# CAM\n",
        "model_conv.eval()\n",
        "y_pred_cam = torch.argmax(soft(model_conv(x)),axis=1)\n",
        "\n",
        "activacion = model_conv.features(x)\n",
        "act_shape = activacion.shape\n",
        "activacion = torch.reshape(activacion, (act_shape[1],act_shape[2],act_shape[3]))\n",
        "\n",
        "heatmap_gato = torch.zeros((activacion.shape[1],activacion.shape[2])).to(device)\n",
        "heatmap_perro = torch.zeros((activacion.shape[1],activacion.shape[2])).to(device)\n",
        "\n",
        "for p in model_conv.fc.parameters():\n",
        "  for i in range(activacion.shape[0]):\n",
        "    heatmap_gato = heatmap_gato + activacion[i]*p[0][i]\n",
        "    heatmap_perro = heatmap_perro + activacion[i]*p[1][i]\n",
        "\n",
        "# Normalizamos\n",
        "maximo_ambos = max(torch.max(heatmap_gato), torch.max(heatmap_perro))\n",
        "minimo_ambos = min(torch.min(heatmap_gato), torch.min(heatmap_perro))\n",
        "print(f\"MAXIMO: {maximo_ambos}\")\n",
        "print(f\"MINIMO: {minimo_ambos}\")\n",
        "\n",
        "heatmap_gato = (heatmap_gato-minimo_ambos)/(abs(maximo_ambos-minimo_ambos))\n",
        "heatmap_perro = (heatmap_perro-minimo_ambos)/(abs(maximo_ambos-minimo_ambos))\n",
        "\n",
        "print(heatmap_perro.size())\n",
        "\n",
        "# Grad-CAM\n",
        "\"\"\"\n",
        "Hay que hacer un modelo que tenga todas las capas de vgg16 más una extra\n",
        "que sea una lineal que saque 2 características.\n",
        "Quizás se pueda hacer esto:\n",
        "model_vgg.fc= nn.Linear(input_features=model_vgg.classifier.output_features, output_features=2)\n",
        "\n",
        "no vale porque tienes que redefinir el forward. Creo q es más rápido definir un nuevo modelo\n",
        "\n",
        "\n",
        "y entrenar este modelo\n",
        "\"\"\"\n",
        "model_vgg.to(device)\n",
        "model_vgg.eval()\n",
        "\n",
        "print(model_vgg)\n",
        "y_pred_grad= torch.argmax(soft(model_vgg(x)),axis=1)\n",
        "\n",
        "\n",
        "grad_activation = model_vgg.avgpool(model_vgg.features(x)) # tiene le requires_grad=True\n",
        "grad_act_shape = grad_activation.shape\n",
        "grad_activation =  torch.reshape(grad_activation, (1, grad_act_shape[1]*grad_act_shape[2]*grad_act_shape[3]))\n",
        "grad_activation.retain_grad() # Enables this Tensor to have their grad populated during backward().\n",
        "Y_grad = model_vgg.classifier(grad_activation)\n",
        "Y_grad[0][100].backward()\n",
        "\n",
        "print(f\"GRAD_ACT:{grad_activation.grad}\")\n",
        "\n",
        "\n",
        "###########################\n",
        "#\n",
        "#   HACEMOS PLOT\n",
        "#\n",
        "##########################\n",
        "\n",
        "x_plot=torch.reshape(x.cpu(), (3,224,224)).permute(1,2,0).numpy()\n",
        "hm_g = heatmap_gato.cpu().detach().numpy()\n",
        "hm_d = heatmap_perro.cpu().detach().numpy()\n",
        "\n",
        "resg = cv2.resize(hm_g*225, dsize=(224, 224), interpolation=cv2.INTER_CUBIC)\n",
        "resd = cv2.resize(hm_d*225, dsize=(224, 224), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "\n",
        "plt.imshow(x_plot)\n",
        "plt.show()\n",
        "\n",
        "print(f\"y:{y_pred_cam[0]}\")\n",
        "if y_pred_cam[0]==0:\n",
        "  print(\"----------- ES UN GATO -----------\")\n",
        "  print(f\"real: {'gato' if classes[0]==0 else 'perro'}\")\n",
        "    \n",
        "  plt.imshow(x_plot)\n",
        "  plt.imshow(resg,alpha=0.5, cmap=plt.get_cmap('seismic'))\n",
        "  plt.show()\n",
        "\n",
        "else:\n",
        "  print(\"----------- ES UN PERRO -----------\")\n",
        "  print(f\"real: {'gato' if classes[0]==0 else 'perro'}\")\n",
        "\n",
        "  plt.imshow(x_plot)\n",
        "  plt.imshow(resd,alpha=0.5, cmap=plt.get_cmap('seismic'))\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "print(f\"c={classes}\")\n",
        "print(f\"PREDICCIÓN: {soft(model_conv(x))}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "ypPqj6SnfQXf",
        "outputId": "59e60af4-d2f0-428f-9768-def8673cab3d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-49ee7d4a029c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# CAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0my_pred_cam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mactivacion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'soft' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t1=torch.tensor(1., requires_grad=True)\n",
        "t2=torch.tensor(2., requires_grad=True)\n",
        "t3=torch.tensor(3., requires_grad=True)\n",
        "################################################\n",
        "\n",
        "t4 = t1+t2\n",
        "t5=t4*t3\n",
        "\n",
        "t5.backward()\n",
        "print(t3.grad)\n",
        "################################################\n",
        "\n",
        "t1.grad=torch.zeros_like(t1)\n",
        "t2.grad=torch.zeros_like(t2)\n",
        "t3.grad=torch.zeros_like(t3)\n",
        "\n",
        "t6= (t3+t1)\n",
        "t7= t6*t2\n",
        "\n",
        "t7.backward()\n",
        "print(t3.grad)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jikIufUas34R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}