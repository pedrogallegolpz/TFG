{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "187defad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases:  ['Benign', 'Pathological']\n",
      "Train image size: 6366\n",
      "Validation image size: 1591\n",
      "Test image size: 4158\n",
      "\n",
      " CAM\n",
      "model_cam loaded\n",
      "Epoch 0/2\n",
      "----------\n",
      "[1, 199] loss: 0.128\n",
      "[1, 399] loss: 0.135\n",
      "[1, 599] loss: 0.132\n",
      "train Loss: 0.1308 Acc: 0.9537\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0930 Acc: 0.9711\n",
      "New best model found!\n",
      "New record loss: 0.09304708566242685, previous record loss: 0.10541678795919975\n",
      "\n",
      "Epoch 1/2\n",
      "----------\n",
      "[2, 199] loss: 0.169\n",
      "[2, 399] loss: 0.139\n",
      "[2, 599] loss: 0.120\n",
      "train Loss: 0.1111 Acc: 0.9617\n",
      "\n",
      "val Loss: 0.0669 Acc: 0.9818\n",
      "New best model found!\n",
      "New record loss: 0.06692599283799407, previous record loss: 0.09304708566242685\n",
      "\n",
      "Epoch 2/2\n",
      "----------\n",
      "[3, 199] loss: 0.058\n",
      "[3, 399] loss: 0.062\n",
      "[3, 599] loss: 0.066\n",
      "train Loss: 0.0624 Acc: 0.9796\n",
      "\n",
      "val Loss: 0.0526 Acc: 0.9830\n",
      "New best model found!\n",
      "New record loss: 0.05264532720834886, previous record loss: 0.06692599283799407\n",
      "\n",
      "Training complete in 3m 55s\n",
      "Best val Acc: 0.9830 Best val loss: 0.0526\n",
      "model_cam guardado\n",
      "\n",
      " GRADCAM\n",
      "model_gradcam loaded\n",
      "Epoch 0/2\n",
      "----------\n",
      "[1, 199] loss: 0.147\n",
      "[1, 399] loss: 0.143\n",
      "[1, 599] loss: 0.149\n",
      "train Loss: 0.1389 Acc: 0.9548\n",
      "\n",
      "val Loss: 0.0859 Acc: 0.9661\n",
      "New best model found!\n",
      "New record loss: 0.08587948539644646, previous record loss: 0.08790952179406487\n",
      "\n",
      "Epoch 1/2\n",
      "----------\n",
      "[2, 199] loss: 0.073\n",
      "[2, 399] loss: 0.084\n",
      "[2, 599] loss: 0.081\n",
      "train Loss: 0.0842 Acc: 0.9717\n",
      "\n",
      "val Loss: 0.0634 Acc: 0.9761\n",
      "New best model found!\n",
      "New record loss: 0.06340973850604238, previous record loss: 0.08587948539644646\n",
      "\n",
      "Epoch 2/2\n",
      "----------\n",
      "[3, 199] loss: 0.087\n",
      "[3, 399] loss: 0.076\n",
      "[3, 599] loss: 0.078\n",
      "train Loss: 0.0774 Acc: 0.9769\n",
      "\n",
      "val Loss: 0.1003 Acc: 0.9654\n",
      "\n",
      "Training complete in 4m 7s\n",
      "Best val Acc: 0.9761 Best val loss: 0.0634\n",
      "model_gradcam guardado\n",
      "\n",
      " GRADCAMPP\n",
      "model_gradcampp loaded\n",
      "Epoch 0/2\n",
      "----------\n",
      "[1, 199] loss: 0.349\n",
      "[1, 399] loss: 0.341\n",
      "[1, 599] loss: 0.334\n",
      "train Loss: 0.3293 Acc: 0.8682\n",
      "\n",
      "val Loss: 0.2827 Acc: 0.8831\n",
      "New best model found!\n",
      "New record loss: 0.2826915975264376, previous record loss: 0.3373971389715474\n",
      "\n",
      "Epoch 1/2\n",
      "----------\n",
      "[2, 199] loss: 0.241\n",
      "[2, 399] loss: 0.250\n",
      "[2, 599] loss: 0.246\n",
      "train Loss: 0.2552 Acc: 0.9056\n",
      "\n",
      "val Loss: 0.1843 Acc: 0.9340\n",
      "New best model found!\n",
      "New record loss: 0.18430746247901772, previous record loss: 0.2826915975264376\n",
      "\n",
      "Epoch 2/2\n",
      "----------\n",
      "[3, 199] loss: 0.191\n",
      "[3, 399] loss: 0.224\n",
      "[3, 599] loss: 0.214\n",
      "train Loss: 0.2050 Acc: 0.9284\n",
      "\n",
      "val Loss: 0.2199 Acc: 0.9126\n",
      "\n",
      "Training complete in 4m 9s\n",
      "Best val Acc: 0.9340 Best val loss: 0.1843\n",
      "model_gradcampp guardado\n",
      "\n",
      " SMOOTHGRADCAMPP\n",
      "model_smoothgradcampp loaded\n",
      "Epoch 0/2\n",
      "----------\n",
      "[1, 199] loss: 0.286\n",
      "[1, 399] loss: 0.277\n",
      "[1, 599] loss: 0.276\n",
      "train Loss: 0.2718 Acc: 0.8987\n",
      "\n",
      "val Loss: 0.2009 Acc: 0.9258\n",
      "New best model found!\n",
      "New record loss: 0.20094266291040433, previous record loss: 0.37548586457387073\n",
      "\n",
      "Epoch 1/2\n",
      "----------\n",
      "[2, 199] loss: 0.207\n",
      "[2, 399] loss: 0.201\n",
      "[2, 599] loss: 0.215\n",
      "train Loss: 0.2051 Acc: 0.9237\n",
      "\n",
      "val Loss: 0.1785 Acc: 0.9359\n",
      "New best model found!\n",
      "New record loss: 0.17849254038297271, previous record loss: 0.20094266291040433\n",
      "\n",
      "Epoch 2/2\n",
      "----------\n",
      "[3, 199] loss: 0.151\n",
      "[3, 399] loss: 0.150\n",
      "[3, 599] loss: 0.160\n",
      "train Loss: 0.1527 Acc: 0.9472\n",
      "\n",
      "val Loss: 0.0937 Acc: 0.9679\n",
      "New best model found!\n",
      "New record loss: 0.09373259365849552, previous record loss: 0.17849254038297271\n",
      "\n",
      "Training complete in 4m 9s\n",
      "Best val Acc: 0.9679 Best val loss: 0.0937\n",
      "model_smoothgradcampp guardado\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.modules.pooling import AdaptiveAvgPool2d\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import abc  # For implementing abstract methods\n",
    "\n",
    "import CAM.cam\n",
    "from utils import train_model\n",
    "import json\n",
    "import math\n",
    "from torchvision.transforms import Resize\n",
    "# Reset CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "path_guardado_modelos = 'modelos/'\n",
    "os.makedirs(path_guardado_modelos, exist_ok=True)\n",
    "\n",
    "def get_data_transforms():\n",
    "    # Just normalization for validation\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomRotation(5),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomResizedCrop(224, scale=(0.96, 1.0), ratio=(0.95, 1.05)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize([224,224]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize([224,224]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    return data_transforms\n",
    "\n",
    "def load_data(path, batch_size=8, split_size=0.2):# Data augmentation and normalization for training\n",
    "    data_transforms = get_data_transforms()\n",
    "    \n",
    "    data_dir = path\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                              data_transforms[x])\n",
    "                      for x in ['train', 'test']}\n",
    "    #print(image_datasets[\"train\"].imgs)\n",
    "\n",
    "    dataset_sizes = {\n",
    "                        \"train\": int(len(image_datasets[\"train\"])-np.floor(len(image_datasets[\"train\"])*split_size)),\n",
    "                        \"val\": int(np.floor(len(image_datasets[\"train\"])*split_size)),\n",
    "                        \"test\": len(image_datasets[\"test\"])\n",
    "                    }\n",
    "\n",
    "    # Creamos el conjunto de validaci√≥n\n",
    "    indices = np.array(range(dataset_sizes['train']+dataset_sizes['val']))\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[dataset_sizes['val']:], indices[:dataset_sizes['val']]\n",
    "    \n",
    "    # Creating PT data samplers and loaders:\n",
    "    train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(val_indices)\n",
    "    \n",
    "    dataloaders = {\n",
    "                    \"train\": torch.utils.data.DataLoader(image_datasets[\"train\"], \n",
    "                                                         batch_size=batch_size,\n",
    "                                                         sampler=train_sampler,\n",
    "                                                         num_workers=2),\n",
    "                    \"val\": torch.utils.data.DataLoader(image_datasets[\"train\"], \n",
    "                                                         batch_size=batch_size,\n",
    "                                                         sampler=valid_sampler,\n",
    "                                                         num_workers=2),\n",
    "                    \"test\": torch.utils.data.DataLoader(image_datasets[\"test\"], \n",
    "                                                         batch_size=batch_size,\n",
    "                                                         shuffle=True,\n",
    "                                                         num_workers=2)      \n",
    "                    }\n",
    "    \n",
    "    \n",
    "    # Test\n",
    "    dataset_test = datasets.ImageFolder(os.path.join(data_dir, 'test'), data_transforms['test'])\n",
    "    dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, \n",
    "                                                  shuffle=True, num_workers=1)\n",
    "    \n",
    "    assert(image_datasets['train'].classes==image_datasets['test'].classes)\n",
    "    \n",
    "    class_names = image_datasets['train'].classes\n",
    "    print(\"Clases: \", class_names) \n",
    "    print(f'Train image size: {dataset_sizes[\"train\"]}')\n",
    "    print(f'Validation image size: {dataset_sizes[\"val\"]}')\n",
    "    print(f'Test image size: {dataset_sizes[\"test\"]}')\n",
    "    \n",
    "    return dataloaders, dataset_sizes, dataloader_test\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('DEVICE: ', device)\n",
    "dataloaders, dataset_sizes, dataloader_test = load_data(r'..\\..\\SICAPv1\\299_patch_impar')\n",
    "\n",
    "model_vgg = models.vgg16(pretrained=True)\n",
    "\n",
    "models = {\n",
    "            'cam': {\n",
    "                        'model':CAM.cam.CAM_model(model_vgg, D_out=2),\n",
    "                        'best_values': {'loss': math.inf, 'acc':0.}\n",
    "                    },\n",
    "            'gradcam':{\n",
    "                        'model':CAM.cam.GradCAM_model(model_vgg, D_out=2),\n",
    "                        'best_values': {'loss': math.inf, 'acc':0.}\n",
    "                    },\n",
    "            'gradcampp':{\n",
    "                        'model':CAM.cam.GradCAMpp_model(model_vgg, D_out=2),\n",
    "                        'best_values': {'loss': math.inf, 'acc':0.}\n",
    "                    },\n",
    "            'smoothgradcampp':{\n",
    "                        'model':CAM.cam.SmoothGradCAMpp_model(model_vgg, D_out=2),\n",
    "                        'best_values': {'loss': math.inf, 'acc':0.}\n",
    "                    },\n",
    "}\n",
    "\n",
    "\n",
    "for name in models.keys():\n",
    "    print('\\n', name.upper())\n",
    "    # Load model\n",
    "    try:\n",
    "        models[f'{name}']['model'] = torch.load(path_guardado_modelos+f\"model_{name}.pth\").to(device)\n",
    "        with open(path_guardado_modelos+f'dic_best_values_model_{name}.json') as f_dic:\n",
    "             models[f'{name}']['best_values'] = json.load(f_dic)\n",
    "        print(f\"model_{name} loaded\")\n",
    "    except:\n",
    "        print(f\"model_{name} not found\")\n",
    "    \n",
    "    # Cogemos el optimizador y el criterio de aprendizaje\n",
    "    optimizer = optim.SGD(models[f'{name}']['model'].parameters(), lr=0.001, momentum=0.9)\n",
    "    criterion = torch.nn.CrossEntropyLoss() \n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    # Entrenamos\n",
    "    model_gradcam, best_val_loss, best_val_acc = train_model(models[f'{name}']['model'],\n",
    "                                                          models[f'{name}']['best_values'],\n",
    "                                                          dataloaders, \n",
    "                                                          dataset_sizes,\n",
    "                                                          criterion,\n",
    "                                                          optimizer,\n",
    "                                                          exp_lr_scheduler,\n",
    "                                                          num_epochs = 0)\n",
    "\n",
    "    # Guardamos el mejor modelo del entrenamiento\n",
    "    torch.save(models[f'{name}']['model'], path_guardado_modelos+f\"model_{name}.pth\")\n",
    "    \n",
    "    models[f'{name}']['best_values'] = {'loss': best_val_loss, 'acc': best_val_acc.item()}\n",
    "    with open(path_guardado_modelos+f'dic_best_values_model_{name}.json','w') as f_dic:\n",
    "        json.dump(models[f'{name}']['best_values'], f_dic)\n",
    "    \n",
    "    print(f'model_{name} guardado')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4993d18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
