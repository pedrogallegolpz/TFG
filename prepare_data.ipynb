{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a12ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "# Def dimension of new images\n",
    "DIM = 224\n",
    "umbral_tejido = 0.35 # 10% de tejido al menos en la imagen para que sea guardada\n",
    "umbral_patologico = 0.25 # 10% de cancer al menos en la imagen para que sea guardada\n",
    "\n",
    "# Def  paths\n",
    "path_SICAP = r'../SICAPv1'\n",
    "path_SICAP_512 = path_SICAP + r'/512_patch'\n",
    "path_SICAP_1024 = path_SICAP + r'/1024_patch'\n",
    "\n",
    "path_dataset_read = path_SICAP_512                            # lectura\n",
    "path_dataset_write_par = path_SICAP + f'/{DIM}_patch_par'     # Escritura\n",
    "path_dataset_write_impar = path_SICAP + f'/{DIM}_patch_impar' # Escritura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da6716a",
   "metadata": {},
   "source": [
    "### Información sobre el Dataset\n",
    "SICAPv1 is a public patch-wise database composed by 78 histological Whole Slide Images (WSI) of the prostate. These images were collected by specialists of the Hospital Clínico Universitario de Valencia. \n",
    "\n",
    "The use of SICAPv1 database is restricted to research pourposes. Please cite SICAPv1 in your publications if it helps your research:\n",
    "\n",
    "Ángel E. Esteban, Miguel López-Lara, Adrián Colomer, María A. Sales, Rafael Molina and Valery Naranjo, \"A new optical density granulometry-based descriptor for the classification of prostate histological images using shallow and deep gaussian processes\"\n",
    "\n",
    "SICAPv1 is composed  of of 78 WSI: 18 correspond to benign prostate tissue biopsies (negative class) and 60 to pathological prostate tissue biopsies (positive class). This dataset was divided into two subsets, 60 WSI (17 benign and 43 pathological) were used to learn the models and the remaining 18 to test them. The 43 pathological WSI are distributed as follows: 18 WSI diagnosed as grade 3, 15 WSI catalogued as grade 4 and the remaining 10 images were marked as grade 5 by the pathologists.\n",
    "\n",
    "In order to automatically analyse these gigapixel images,  the images weredownsampled from 40� to 10� and divided in patches with a 50% overlap.  To test the influence of the patch size, different sizes were selected: 512^2 and 1024^2,resulting on the two different datasets detailed in the following table:\n",
    "\n",
    "\t\tBenign\tGrade3\tGrade4\tGrade5\tMalign\n",
    "\n",
    "#WSIs\t\t  17\t\t  18\t\t  15\t\t  10\t\t  43\n",
    "\n",
    "#512 patch\t 6725\t\t  380\t\t  589\t\t  173\t\t 1142\n",
    "\n",
    "#1024 patch 1909\t\t  113\t\t  181\t\t  50\t\t  344\n",
    "\n",
    "\n",
    "SICAP1 database is composed of two external folders (one containing the patches of size 512^2 and the other one containing the patches of size 1024^2. Inside 512^2 folder two subfolders containing train and test partition can be found. In 1024^2 case, the test images are not available because this case is outperformed by 512^2 patches in validation. Inside train/test folders, the data is divided taking into account the class (i.e. benign and pathological). Inside each WSI identifier, five different subfolders can be found:\n",
    "\n",
    "- Annotation: It contains the RGB patches marked as pathological by the experts (in the case of benign samples this folder is empty).\n",
    "\n",
    "- AnnotationMask: It contains the binary masks belonging to the patches marked as pathological by the experts (in the case of benign samples this folder is empty)\n",
    "\n",
    "- NoAnnotation: It corresponds to the RGB patches of a WSI without annotation.\n",
    "\n",
    "- NoAnnotationMask: It contains the tissue masks (i.e. mask discerning tissue and background) from the NoAnnotated patches.\n",
    "\n",
    "- TissueMask: It contains the tissue masks from the patches containing annotation. (EN TEST NO HAY)\n",
    "\n",
    "If you have any doubt about the distrubution of the images, do not hesitate to contact us:\n",
    "\n",
    "cvblab@i3b.upv.es\n",
    "\n",
    "\n",
    "=============================================================================================\n",
    "\n",
    "Notas: \n",
    "1.- Nombre de archivos: 16B0006668_Block_Region_1_0_0_xini_14356_yini_78832\n",
    "\t\tEl primer número se refiere a la región (no pertenecen a la misma imagen inicial). El segundo número pertenece al desplazamiento vertical, y el tercero al horizontal.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ead8cc",
   "metadata": {},
   "source": [
    "### Funciones para procesar el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "214b73ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_patches(file, path_wsi, phase, label, wsi, patch_type_mask, numero):\n",
    "    if numero%2==0:\n",
    "        path_write = os.path.join(path_dataset_write_par, phase, label) # falta el patch_type[Mask] y file\n",
    "        path_write_mask = os.path.join(path_dataset_write_par, phase+'_mask', label) # falta el patch_type[Mask] y file        \n",
    "        path_write_pathological_noannotation = os.path.join(path_dataset_write_par, phase+'_pathological_noannotation', label) # falta el patch_type[Mask] y file        \n",
    "    else:\n",
    "        path_write = os.path.join(path_dataset_write_impar, phase, label) # falta el patch_type[Mask] y file\n",
    "        path_write_mask = os.path.join(path_dataset_write_impar, phase+'_mask', label) # falta el patch_type[Mask] y file\n",
    "        path_write_pathological_noannotation = os.path.join(path_dataset_write_impar, phase+'_pathological_noannotation', label) # falta el patch_type[Mask] y file\n",
    "    \n",
    "        \n",
    "    \n",
    "    umbral = umbral_tejido if 'noannotation' in patch_type_mask else umbral_patologico\n",
    "    \n",
    "    \n",
    "    img_mask = cv2.imread(path_wsi+f'/{patch_type_mask}/{file}')\n",
    "    img_mask[img_mask<=20]=0\n",
    "    img_mask[img_mask>20]=255\n",
    "\n",
    "    file2 = file.replace('_b.jpg','.jpg')\n",
    "    patch_type = patch_type_mask.replace('Mask','')\n",
    "    # Pueden pasar que: \n",
    "    #     1) lo lea bien\n",
    "    #     2) esté en la carpeta contraria (en test por ejemplo\n",
    "    #        la carpeta 'annotation' está vacía y todas las imágenes,\n",
    "    #        están sin filtrar en noannotation)\n",
    "\n",
    "    path_img = path_wsi+f'/{patch_type}/{file2}'   # Tiene el nombre \n",
    "    if not os.path.exists(path_img): # Si no existe es que es benigno\n",
    "        path_img = path_wsi+f'/no{patch_type}/{file2}'   # Tiene el nombre exacto\n",
    "\n",
    "        if not os.path.exists(path_img):\n",
    "            print(f'ERROR: {path_img}, no existe el archivo')\n",
    "\n",
    "    img = cv2.imread(path_img)\n",
    "\n",
    "    assert(not(img is None))\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    [:DIM][:DIM]                                   # (0,0)\n",
    "    [:DIM][256-DIM/2:256+DIM/2]                    # (0,1)\n",
    "    [:DIM][-DIM:]                                  # (0,2)\n",
    "    \n",
    "    [256-DIM/2:256+DIM/2][:DIM]                    # (1,0)\n",
    "    [256-DIM/2:256+DIM/2][256-DIM/2:256+DIM/2]     # (1,1)\n",
    "    [256-DIM/2:256+DIM/2][-DIM:]                   # (1,2)\n",
    "    \n",
    "    [-DIM:][:DIM]                                  # (2,0)\n",
    "    [-DIM:][256-DIM/2:256+DIM/2]                   # (2,1)\n",
    "    [-DIM:][-DIM:]                                 # (2,2)\n",
    "    \"\"\"    \n",
    "    \n",
    "    inicios_fila = [0,0,0, int(256-DIM/2), int(256-DIM/2), int(256-DIM/2),-DIM,-DIM,-DIM]\n",
    "    finales_fila = [DIM, DIM, DIM, int(256+DIM/2), int(256+DIM/2), int(256+DIM/2), None, None, None]\n",
    "    \n",
    "    inicios_columna = [0, int(256-DIM/2), -DIM, 0, int(256-DIM/2), -DIM, 0, int(256-DIM/2), -DIM]\n",
    "    finales_columna = [DIM, int(256+DIM/2), None, DIM, int(256+DIM/2), None, DIM, int(256+DIM/2), None]\n",
    "    \n",
    "    for i, f_ini,f_fin, c_ini,c_fin in zip(range(9),inicios_fila, finales_fila, inicios_columna, finales_columna):\n",
    "        parche = img[f_ini:f_fin, c_ini:c_fin, :]\n",
    "        parchemask = img_mask[f_ini:f_fin, c_ini:c_fin, :]\n",
    "        \n",
    "        if parchemask.mean()/255.0>umbral:\n",
    "            # Nombre\n",
    "            parche_name = f'__{i}.'.join(file2.split('.')) # sin el _b que tienen algunas imágenes\n",
    "            \n",
    "            # Imagen original\n",
    "            if ('annotation'==patch_type and label=='Pathological') or ('noannotation'==patch_type and label=='Benign'):\n",
    "                # Guardamos en los directorios de donde generamos el dataset para tratarlo\n",
    "                os.makedirs(os.path.join(path_write).replace('\\\\','/'), exist_ok=True)\n",
    "                cv2.imwrite(os.path.join(path_write, parche_name).replace('\\\\','/'), parche) \n",
    "                \n",
    "                # Máscara annotation/noannotation\n",
    "                os.makedirs(os.path.join(path_write_mask).replace('\\\\','/'), exist_ok=True)\n",
    "                cv2.imwrite(os.path.join(path_write_mask, parche_name).replace('\\\\','/'), parchemask) \n",
    "            else:\n",
    "                # Guardamos en los directorios de _info\n",
    "                os.makedirs(os.path.join(path_write_pathological_noannotation, patch_type).replace('\\\\','/'), exist_ok=True)\n",
    "                cv2.imwrite(os.path.join(path_write_pathological_noannotation, patch_type, parche_name).replace('\\\\','/'), parche)\n",
    "                \n",
    "                # Máscara pathological noannotation\n",
    "                os.makedirs(os.path.join(path_write_pathological_noannotation, patch_type_mask).replace('\\\\','/'), exist_ok=True)\n",
    "                cv2.imwrite(os.path.join(path_write_pathological_noannotation, patch_type_mask, parche_name).replace('\\\\','/'), parchemask) \n",
    "                \n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cf3359",
   "metadata": {},
   "source": [
    "### Lectura de las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88052da9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'train']\n",
      "../SICAPv1/224_patch_par\\train\\Pathological\n",
      "../SICAPv1/224_patch_impar\\train\\Pathological\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(path_dataset_read))\n",
    "\n",
    "def generate_dataset():\n",
    "    for root, dirs, files in os.walk(path_dataset_read, topdown=False):\n",
    "        for directory in dirs:\n",
    "            if 'annotationmask' not in directory.lower():\n",
    "                continue\n",
    "\n",
    "            root = root.replace('\\\\','/')\n",
    "            root_std = os.path.join(root, directory).replace('\\\\','/')\n",
    "            split = root_std.split('/')[-4:-1]\n",
    "\n",
    "            phase = split[0]               # test or train\n",
    "            label = split[1]               # Pathological or Bening\n",
    "            wsi   = split[2]               # id\n",
    "            patch_type = directory         # annotationMask, noannotationMask\n",
    "            print(split)         \n",
    "            \n",
    "            for file in os.listdir(root_std):\n",
    "                if '.jpg' not in file.lower():\n",
    "                    continue\n",
    "\n",
    "                vertical = float(file.split('_xini')[0].split('_')[-1])\n",
    "                horizontal = float(file.split('_xini')[0].split('_')[-2])\n",
    "                \n",
    "                # Necesitamos que los dos sean pares, o los dos impares\n",
    "                if (vertical+horizontal)%2:\n",
    "                    # Como son del mismo tipo, nos bastaría saber si uno es impar o par\n",
    "                    # para meterlos en su clase\n",
    "                    par_impar = horizontal%2\n",
    "                    \n",
    "                    generate_new_patches(file=file, \n",
    "                                         path_wsi = root, \n",
    "                                         phase=phase, \n",
    "                                         label=label, \n",
    "                                         wsi=wsi,\n",
    "                                         patch_type_mask=patch_type,\n",
    "                                         numero=par_impar)\n",
    "           \n",
    "        \n",
    "def generate_validation_set(split_size = 0.2):\n",
    "    for path_dataset_write_ in [path_dataset_write_par, path_dataset_write_impar]:\n",
    "        for label in ['Benign', 'Pathological']:\n",
    "            if os.path.exists(os.path.join(path_dataset_write_,'val',label)):\n",
    "                continue\n",
    "\n",
    "            # Cogemos los archivos y los desordenamos\n",
    "            files = os.listdir(os.path.join(path_dataset_write_,'train',label))\n",
    "            np.random.shuffle(files)\n",
    "\n",
    "            print(os.path.join(path_dataset_write_,'train', label))\n",
    "            # Creamos las rutas\n",
    "            os.makedirs(os.path.join(path_dataset_write_,'val', label))\n",
    "            os.makedirs(os.path.join(path_dataset_write_,'val_mask', label))\n",
    "            \n",
    "            # Movemos\n",
    "            size_validation = int(split_size * len(files))\n",
    "            for file in files[:size_validation]:\n",
    "                # Image\n",
    "                src = os.path.join(path_dataset_write_, 'train', label, file)\n",
    "                dest = os.path.join(path_dataset_write_, 'val', label, file)\n",
    "                shutil.move(src, dest)\n",
    "\n",
    "                # Mask\n",
    "                src = os.path.join(path_dataset_write_, 'train'+'_mask', label, file)\n",
    "                dest = os.path.join(path_dataset_write_, 'val'+'_mask', label, file)\n",
    "                shutil.move(src, dest)\n",
    "\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "#generate_dataset()\n",
    "generate_validation_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a55953a",
   "metadata": {},
   "source": [
    "### Balanceo de clases: comprobación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b960d64",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUIDADO:  ../SICAPv1/512_patch\n",
      "He entrado estas veces:  79\n",
      "ORIGINAL\n",
      "\tTEST\t{'Benign': 5681, 'Pathological': 3907}\n",
      "\tBenign: 0.592511472674176\n",
      "\tPathological: 0.40748852732582397\n",
      "\t==================================================\n",
      "\tTRAIN\t{'Benign': 28472, 'Pathological': 3697}\n",
      "\tBenign: 0.8850756939911094\n",
      "\tPathological: 0.11492430600889054\n",
      "\n",
      "########################################\n",
      "########################################\n",
      "He entrado estas veces:  0\n",
      "PAR\n",
      "\tTEST\t{'Benign': 4014, 'Pathological': 3322}\n",
      "\tBenign: 0.547164667393675\n",
      "\tPathological: 0.45283533260632497\n",
      "\t==================================================\n",
      "\tTRAIN\t{'Benign': 11387, 'Pathological': 2929}\n",
      "\tBenign: 0.7954037440625873\n",
      "\tPathological: 0.20459625593741268\n",
      "\n",
      "########################################\n",
      "########################################\n",
      "He entrado estas veces:  0\n",
      "IMPAR\n",
      "\tTEST\t{'Benign': 3751, 'Pathological': 3233}\n",
      "\tBenign: 0.5370847651775487\n",
      "\tPathological: 0.46291523482245134\n",
      "\t==================================================\n",
      "\tTRAIN\t{'Benign': 11292, 'Pathological': 2906}\n",
      "\tBenign: 0.795323284969714\n",
      "\tPathological: 0.20467671503028595\n",
      "\n",
      "########################################\n",
      "########################################\n"
     ]
    }
   ],
   "source": [
    "for path_read, dataset_par_impar in zip([path_dataset_read, path_dataset_write_par, path_dataset_write_impar], [\"ORIGINAL\", \"PAR\", \"IMPAR\"]):\n",
    "    test_balanceo = {'Benign':0, 'Pathological':0}\n",
    "    train_balanceo = {'Benign':0, 'Pathological':0}\n",
    "    \n",
    "    he_entrado_veces = 0\n",
    "    for root, dirs, files in os.walk(path_read):\n",
    "        root_std = root.replace('\\\\','/')\n",
    "        \n",
    "        if dataset_par_impar == \"ORIGINAL\":\n",
    "            try:\n",
    "                curr_dir = root_std.split('/')[-1]\n",
    "                phase = root_std.split('/')[-4]\n",
    "                label = root_std.split('/')[-3]\n",
    "            except:\n",
    "                print(\"CUIDADO: \",root_std)\n",
    "                continue\n",
    "                \n",
    "            \n",
    "            if ('annotationMask'==curr_dir and label=='Pathological') or ('noannotation'==curr_dir and label=='Benign'):\n",
    "                if 'train'==phase:\n",
    "                    he_entrado_veces += 1\n",
    "                    train_balanceo[f'{label}'] += len(files)\n",
    "                elif phase=='test':\n",
    "                    he_entrado_veces += 1\n",
    "                    test_balanceo[f'{label}'] += len(files)\n",
    "        else:\n",
    "            phase = root_std.split('/')[-2]\n",
    "            label = root_std.split('/')[-1]\n",
    "\n",
    "            if phase=='train':\n",
    "                train_balanceo[f'{label}'] += len(files)\n",
    "            elif phase=='test':\n",
    "                 test_balanceo[f'{label}'] += len(files)\n",
    "\n",
    "    print(\"He entrado estas veces: \", he_entrado_veces)\n",
    "    print(dataset_par_impar)\n",
    "    print(f\"\\tTEST\\t{test_balanceo}\")\n",
    "    try:\n",
    "        print(f\"\\tBenign: {test_balanceo['Benign']/(test_balanceo['Benign']+test_balanceo['Pathological'])}\\n\\tPathological: {test_balanceo['Pathological']/(test_balanceo['Benign']+test_balanceo['Pathological'])}\")\n",
    "    except:\n",
    "        print()\n",
    "    print(\"\\t\"+\"=\"*50)\n",
    "    print(f\"\\tTRAIN\\t{train_balanceo}\")\n",
    "    try:\n",
    "        print(f\"\\tBenign: {train_balanceo['Benign']/(train_balanceo['Benign']+train_balanceo['Pathological'])}\\n\\tPathological: {train_balanceo['Pathological']/(train_balanceo['Benign']+train_balanceo['Pathological'])}\")\n",
    "    except:\n",
    "        print()\n",
    "    print(\"\\n\"+\"#\"*40)\n",
    "    print(\"#\"*40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54e4c99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ['16B0022616_Block_Region_0_0_1_xini_4935_yini_141611__0.jpg'\n",
      " '16B0022616_Block_Region_0_0_1_xini_4935_yini_141611__1.jpg'\n",
      " '16B0022616_Block_Region_0_0_1_xini_4935_yini_141611__2.jpg' ...\n",
      " '16B0028822_Block_Region_5_8_33_xini_41306_yini_10787__3.jpg'\n",
      " '16B0028822_Block_Region_5_8_33_xini_41306_yini_10787__4.jpg'\n",
      " '16B0028822_Block_Region_5_8_33_xini_41306_yini_10787__5.jpg']\n",
      "None\n",
      "\n",
      " ['16B0001851_Block_Region_1_0_1_xini_7827_yini_59786__0.jpg'\n",
      " '16B0001851_Block_Region_1_0_1_xini_7827_yini_59786__1.jpg'\n",
      " '16B0001851_Block_Region_1_0_1_xini_7827_yini_59786__2.jpg' ...\n",
      " '17B0024162_Block_Region_4_8_5_xini_21057_yini_44634__4.jpg'\n",
      " '17B0024162_Block_Region_4_8_5_xini_21057_yini_44634__6.jpg'\n",
      " '17B0024162_Block_Region_4_8_5_xini_21057_yini_44634__7.jpg']\n",
      "None\n",
      "\n",
      " ['18B0006623A_Block_Region_0_0_11_xini_29998_yini_168002__6.jpg'\n",
      " '18B0006623A_Block_Region_0_0_11_xini_29998_yini_168002__7.jpg'\n",
      " '18B0006623A_Block_Region_0_0_13_xini_31022_yini_168002__6.jpg' ...\n",
      " '18B0006623I_Block_Region_8_8_9_xini_13881_yini_52500__5.jpg'\n",
      " '18B0006623I_Block_Region_8_8_9_xini_13881_yini_52500__7.jpg'\n",
      " '18B0006623I_Block_Region_8_8_9_xini_13881_yini_52500__8.jpg']\n",
      "None\n",
      "\n",
      " ['17B0017511_Block_Region_2_10_17_xini_15700_yini_104645__1.jpg'\n",
      " '17B0017511_Block_Region_2_10_17_xini_15700_yini_104645__2.jpg'\n",
      " '17B0017511_Block_Region_2_10_17_xini_15700_yini_104645__4.jpg' ...\n",
      " '18B001071J_Block_Region_4_2_7_xini_30209_yini_60675__2.jpg'\n",
      " '18B001071J_Block_Region_4_2_9_xini_31233_yini_60675__0.jpg'\n",
      " '18B001071J_Block_Region_4_2_9_xini_31233_yini_60675__1.jpg']\n",
      "None\n",
      "\n",
      " ['16B0022616_Block_Region_0_11_2_xini_5959_yini_152875__0.jpg'\n",
      " '16B0022616_Block_Region_0_11_2_xini_5959_yini_152875__1.jpg'\n",
      " '16B0022616_Block_Region_0_11_2_xini_5959_yini_152875__2.jpg' ...\n",
      " '16B0028822_Block_Region_5_7_34_xini_42330_yini_9763__6.jpg'\n",
      " '16B0028822_Block_Region_5_7_34_xini_42330_yini_9763__7.jpg'\n",
      " '16B0028822_Block_Region_5_7_34_xini_42330_yini_9763__8.jpg']\n",
      "None\n",
      "\n",
      " ['16B0001851_Block_Region_1_1_0_xini_6803_yini_60810__1.jpg'\n",
      " '16B0001851_Block_Region_1_1_0_xini_6803_yini_60810__2.jpg'\n",
      " '16B0001851_Block_Region_1_1_2_xini_8851_yini_60810__0.jpg' ...\n",
      " '17B0024162_Block_Region_4_9_4_xini_20033_yini_45658__6.jpg'\n",
      " '17B0024162_Block_Region_4_9_4_xini_20033_yini_45658__7.jpg'\n",
      " '17B0024162_Block_Region_4_9_4_xini_20033_yini_45658__8.jpg']\n",
      "None\n",
      "\n",
      " ['18B0006623A_Block_Region_0_1_0_xini_24366_yini_168514__0.jpg'\n",
      " '18B0006623A_Block_Region_0_1_0_xini_24366_yini_168514__1.jpg'\n",
      " '18B0006623A_Block_Region_0_1_0_xini_24366_yini_168514__2.jpg' ...\n",
      " '18B0006623I_Block_Region_8_9_8_xini_13369_yini_53012__2.jpg'\n",
      " '18B0006623I_Block_Region_8_9_8_xini_13369_yini_53012__4.jpg'\n",
      " '18B0006623I_Block_Region_8_9_8_xini_13369_yini_53012__5.jpg']\n",
      "None\n",
      "\n",
      " ['17B0017511_Block_Region_2_11_14_xini_14164_yini_105157__7.jpg'\n",
      " '17B0017511_Block_Region_2_11_14_xini_14164_yini_105157__8.jpg'\n",
      " '17B0017511_Block_Region_2_11_16_xini_15188_yini_105157__2.jpg' ...\n",
      " '18B001071J_Block_Region_4_1_8_xini_30721_yini_60163__1.jpg'\n",
      " '18B001071J_Block_Region_4_1_8_xini_30721_yini_60163__3.jpg'\n",
      " '18B001071J_Block_Region_4_1_8_xini_30721_yini_60163__4.jpg']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for path_dataset_write_ in [path_dataset_write_par, path_dataset_write_impar]:\n",
    "    for phase in ['train','test']:\n",
    "        for label in ['Benign', 'Pathological']:\n",
    "            a=np.array(os.listdir(os.path.join(path_dataset_write_,phase,label)))\n",
    "            print(\"\\n\",a)\n",
    "            print(np.random.shuffle(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb76cd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adios' 'hola' 'que tal']\n"
     ]
    }
   ],
   "source": [
    "a=np.array([\"hola\", \"que tal\", \"adios\"])\n",
    "np.random.shuffle(a)\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
